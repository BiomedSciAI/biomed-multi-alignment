{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing a new task to MAMMAL\n",
    "\n",
    "Welcome and thanks for joining our journey! \n",
    "\n",
    "\n",
    "This tutorial will serve as a walkthrough guide to create a custom task on the MAMMAL framework to fine-tune our base model [**biomed.omics.bl.sm.ma-ted-458m**](https://huggingface.co/ibm/biomed.omics.bl.sm.ma-ted-458m). \\\n",
    "As a case study we will use our [protein solubility prediction task](https://github.com/BiomedSciAI/biomed-multi-alignment/tree/main?tab=readme-ov-file#protein-solubility-prediction). We will break down the main components so you will be able to create / modify them with your own task and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "Firstly, make sure you have mammal package install. Please follow the [installation guide](https://github.com/BiomedSciAI/biomed-multi-alignment/blob/main/README.md#installation) in the main README file.\n",
    "\n",
    "As for this notebook, we can simply run the next code block (you'll might need to restart the session and re-run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install biomed-multi-alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Picture\n",
    "In order to implement a downstream task for the MAMMAL framework, one should implement a `MammalTask` class instance ([source](https://github.com/BiomedSciAI/biomed-multi-alignment/blob/e56a03e0e9f69e42f919a96def739b78e50a47e5/mammal/task.py#L15)). A `MammalTask` class consists of three main components:\n",
    "1. Data Module - A Lightning data module (`LightningDataModule`) where we load and process the data of the task.\n",
    "2. `data_preprocessing()` method. Which responsible for formatting the input prompt to the model's expected format.\n",
    "3. `process_model_output()` method. Which takes the raw output of the model and translates it into a human-level answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Module\n",
    "We need to create a custom Lightning data module in order to load and prepare the input data for the model. \\\n",
    "In the datamodule we will need to:\n",
    "1. Load the raw dataset from our source (in this example we use TDC's API) - will be done in `load_datasets()` .\n",
    "2. Process the raw data to match the model's prompt structure - will be done in `data_preprocessing()` which we cover after in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from fuse.data.ops.ops_read import OpReadDataframe\n",
    "from fuse.data.datasets.dataset_default import DatasetDefault\n",
    "from fuse.data.pipelines.pipeline_default import PipelineDefault\n",
    "\n",
    "\n",
    "_SOLUBILITY_URL = \"https://zenodo.org/api/records/1162886/files-archive\"\n",
    "\n",
    "\n",
    "def load_datasets(data_path: str) -> dict[str, DatasetDefault]:\n",
    "    \"\"\"\n",
    "    Automatically downloads the data and create dataset iterator for \"train\", \"val\" and \"test\".\n",
    "    paper: https://academic.oup.com/bioinformatics/article/34/15/2605/4938490\n",
    "    Data retrieved from: https://zenodo.org/records/1162886\n",
    "    The benchmark requires classifying protein sequences into binary labels - Soluble or Insoluble (1 or 0).\n",
    "    :param data_path: path to a directory to store the raw data\n",
    "    :return: dictionary that maps fold name \"train\", \"val\" and \"test\" to a dataset iterator\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    raw_data_path = os.path.join(data_path, \"sameerkhurana10-DSOL_rv0.2-20562ad/data\")\n",
    "    if not os.path.exists(raw_data_path):\n",
    "        wget.download(_SOLUBILITY_URL, data_path)\n",
    "        file_path = os.path.join(data_path, \"1162886.zip\")\n",
    "        shutil.unpack_archive(file_path, extract_dir=data_path)\n",
    "        inner_file_path = os.path.join(\n",
    "            data_path, \"sameerkhurana10\", \"DSOL_rv0.2-v0.3.zip\"\n",
    "        )\n",
    "        shutil.unpack_archive(inner_file_path, extract_dir=data_path)\n",
    "        assert os.path.exists(\n",
    "            raw_data_path\n",
    "        ), f\"Error: download complete but {raw_data_path} doesn't exist\"\n",
    "\n",
    "    # read files\n",
    "    df_dict = {}\n",
    "    for set_name in [\"train\", \"val\", \"test\"]:\n",
    "        input_df = pd.read_csv(\n",
    "            os.path.join(raw_data_path, f\"{set_name}_src\"), names=[\"data.protein\"]\n",
    "        )\n",
    "        labels_df = pd.read_csv(\n",
    "            os.path.join(raw_data_path, f\"{set_name}_tgt\"), names=[\"data.label\"]\n",
    "        )\n",
    "        df_dict[set_name] = (input_df, labels_df)\n",
    "\n",
    "    ds_dict = {}\n",
    "    for set_name in [\"train\", \"val\", \"test\"]:\n",
    "        input_df, labels_df = df_dict[set_name]\n",
    "        size = len(labels_df)\n",
    "        print(f\"{set_name} set size is {size}\")\n",
    "        dynamic_pipeline = PipelineDefault(\n",
    "            \"solubility\",\n",
    "            [\n",
    "                (OpReadDataframe(input_df, key_column=None), dict()),\n",
    "                (OpReadDataframe(labels_df, key_column=None), dict()),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        ds = DatasetDefault(sample_ids=size, dynamic_pipeline=dynamic_pipeline)\n",
    "        ds.create()\n",
    "        ds_dict[set_name] = ds\n",
    "\n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size is 62478\n",
      "val set size is 6942\n",
      "test set size is 1999\n",
      "Visualize sample in a tree-fashion\n",
      "--- data\n",
      "------ label -> 1\n",
      "------ initial_sample_id -> 0\n",
      "------ sample_id -> 0\n",
      "------ protein -> GMILKTNLFGHTYQFKSITDVLAKANEEKSGDRLAGVAAESAEERVAAKVVLSKMTLGDLRNNPVVPYETDEVTRIIQDQVNDRIHDSIKNWTVEELREWILDHKTTDADIKRVARGLTSEIIAAVTKLMSNLDLIYGAKKIRVIAHANTTIGLPGTFSARLQPNHPTDDPDGILASLMEGLTYGIGDAVIGLNPVDDSTDSVVRLLNKFEEFRSKWDVPTQTCVLAHVKTQMEAMRRGAPTGLVFQSIAGSEKGNTAFGFDGATIEEARQLALQSGAATGPNVMYFETGQGSELSSDAHFGVDQVTMEARCYGFAKKFDPFLVNTVVGFIGPEYLYDSKQVIRAGLEDHFMGKLTGISMGCDVCYTNHMKADQNDVENLSVLLTAAGCNFIMGIPHGDDVMLNYQTTGYHETATLRELFGLKPIKEFDQWMEKMGFSENGKLTSRAGDASIFLK\n",
      "Visualize sample as a raw flat dictionary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data.initial_sample_id': 0, 'data.sample_id': 0, 'data.protein': 'GMILKTNLFGHTYQFKSITDVLAKANEEKSGDRLAGVAAESAEERVAAKVVLSKMTLGDLRNNPVVPYETDEVTRIIQDQVNDRIHDSIKNWTVEELREWILDHKTTDADIKRVARGLTSEIIAAVTKLMSNLDLIYGAKKIRVIAHANTTIGLPGTFSARLQPNHPTDDPDGILASLMEGLTYGIGDAVIGLNPVDDSTDSVVRLLNKFEEFRSKWDVPTQTCVLAHVKTQMEAMRRGAPTGLVFQSIAGSEKGNTAFGFDGATIEEARQLALQSGAATGPNVMYFETGQGSELSSDAHFGVDQVTMEARCYGFAKKFDPFLVNTVVGFIGPEYLYDSKQVIRAGLEDHFMGKLTGISMGCDVCYTNHMKADQNDVENLSVLLTAAGCNFIMGIPHGDDVMLNYQTTGYHETATLRELFGLKPIKEFDQWMEKMGFSENGKLTSRAGDASIFLK', 'data.label': 1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset - train, validation and test splits\n",
    "ds_dict = load_datasets(\"./data\")\n",
    "\n",
    "# Retrieve and visualize a single sample\n",
    "sample_dict = ds_dict[\"train\"][0]\n",
    "\n",
    "print(\"Visualize sample in a tree-fashion\")\n",
    "sample_dict.print_tree(print_values=True)\n",
    "\n",
    "print(\"Visualize sample as a raw flat dictionary\")\n",
    "sample_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can complete our data module class.\n",
    "\n",
    "##### NOTE\n",
    "The `data_preprocessing` callable function will be implemented and explained in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from fuse.data.utils.collates import CollateDefault\n",
    "from fuse.data.tokenizers.modular_tokenizer.op import ModularTokenizerOp\n",
    "\n",
    "\n",
    "class ProteinSolubilityDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        tokenizer_op: ModularTokenizerOp,\n",
    "        data_preprocessing: callable,\n",
    "        # Default values for simplicity\n",
    "        batch_size: int = 1,\n",
    "        data_path: str = \"./example_solubility_data\",\n",
    "        protein_max_seq_length: int = 1250,\n",
    "        encoder_input_max_seq_len: int = 1260,\n",
    "        labels_max_seq_len: int = 4,\n",
    "    ) -> None:\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            data_path (str): path to the raw data, if not exist, will download the data to the given path.\n",
    "            batch_size (int): batch size\n",
    "            tokenizer_op (ModularTokenizerOp): tokenizer op\n",
    "            encoder_inputs_max_seq_len: max tokenizer sequence length for the encoder inputs,\n",
    "            labels_max_seq_len: max tokenizer sequence length for the labels,\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer_op = tokenizer_op\n",
    "        self.protein_max_seq_length = protein_max_seq_length\n",
    "        self.encoder_input_max_seq_len = encoder_input_max_seq_len\n",
    "        self.labels_max_seq_len = labels_max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.data_preprocessing = data_preprocessing\n",
    "\n",
    "        self.pad_token_id = self.tokenizer_op.get_token_id(\"<PAD>\")\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        self.ds_dict = load_datasets(self.data_path)\n",
    "\n",
    "        task_pipeline = [\n",
    "            (\n",
    "                # Prepare the input string(s) in modular tokenizer input format\n",
    "                self.data_preprocessing,\n",
    "                dict(\n",
    "                    protein_sequence_key=\"data.protein\",\n",
    "                    solubility_label_key=\"data.label\",\n",
    "                    tokenizer_op=self.tokenizer_op,\n",
    "                    protein_max_seq_length=self.protein_max_seq_length,\n",
    "                    encoder_input_max_seq_len=self.encoder_input_max_seq_len,\n",
    "                    labels_max_seq_len=self.labels_max_seq_len,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        for ds in self.ds_dict.values():\n",
    "            ds.dynamic_pipeline.extend(task_pipeline)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        train_loader = DataLoader(\n",
    "            dataset=self.ds_dict[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=CollateDefault(),\n",
    "            shuffle=True,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        val_loader = DataLoader(\n",
    "            self.ds_dict[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=CollateDefault(),\n",
    "        )\n",
    "\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        test_loader = DataLoader(\n",
    "            self.ds_dict[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=CollateDefault(),\n",
    "        )\n",
    "\n",
    "        return test_loader\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader:\n",
    "        return self.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `data_preprocessing()` function\n",
    "\n",
    "\n",
    "This method plays a crucial role in the task's workflow. Here we process the raw data the we load into a prompt that fits the model's pretraining prompt distribution (see paper for more details). \\\n",
    "Besides formatting the prompt as a string, we also tokenize it using our custom modular tokenizer operator. Thus, it is essential to ensure that the raw data is translated into entities that align with the lexicon the model was trained on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from mammal.keys import *  # Import all dictionary static keys\n",
    "\n",
    "\n",
    "def data_preprocessing(\n",
    "    sample_dict: dict,\n",
    "    *,\n",
    "    protein_sequence_key: str,\n",
    "    tokenizer_op: ModularTokenizerOp,\n",
    "    solubility_label_key: int | None = None,\n",
    "    protein_max_seq_length: int = 1250,\n",
    "    encoder_input_max_seq_len: int | None = 1260,\n",
    "    labels_max_seq_len: int | None = 4,\n",
    "    device: str | torch.device = \"cpu\",\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    :param sample_dict: a dictionary with raw data\n",
    "    :param protein_sequence_key: sample_dict key which points to protein sequence\n",
    "    :param solubility_label_key: sample_dict key which points to label\n",
    "    :param protein_max_seq_length: max sequence length of a protein. Will be used to truncate the protein\n",
    "    :param encoder_input_max_seq_len: max sequence length of labels. Will be used to truncate/pad the encoder_input.\n",
    "    :param labels_max_seq_len: max sequence length of labels. Will be used to truncate/pad the labels.\n",
    "    :param tokenizer_op: tokenizer op\n",
    "\n",
    "    \"\"\"\n",
    "    protein_sequence = sample_dict[protein_sequence_key]\n",
    "    solubility_label = sample_dict.get(solubility_label_key, None)\n",
    "\n",
    "    sample_dict[ENCODER_INPUTS_STR] = (\n",
    "        f\"<@TOKENIZER-TYPE=AA><MOLECULAR_ENTITY><MOLECULAR_ENTITY_GENERAL_PROTEIN><SOLUBILITY><SENTINEL_ID_0><@TOKENIZER-TYPE=AA@MAX-LEN={protein_max_seq_length}><SEQUENCE_NATURAL_START>{protein_sequence}<SEQUENCE_NATURAL_END><EOS>\"\n",
    "    )\n",
    "    tokenizer_op(\n",
    "        sample_dict=sample_dict,\n",
    "        key_in=ENCODER_INPUTS_STR,\n",
    "        key_out_tokens_ids=ENCODER_INPUTS_TOKENS,\n",
    "        key_out_attention_mask=ENCODER_INPUTS_ATTENTION_MASK,\n",
    "        max_seq_len=encoder_input_max_seq_len,\n",
    "    )\n",
    "    sample_dict[ENCODER_INPUTS_TOKENS] = torch.tensor(\n",
    "        sample_dict[ENCODER_INPUTS_TOKENS], device=device\n",
    "    )\n",
    "    sample_dict[ENCODER_INPUTS_ATTENTION_MASK] = torch.tensor(\n",
    "        sample_dict[ENCODER_INPUTS_ATTENTION_MASK], device=device\n",
    "    )\n",
    "\n",
    "    if solubility_label is not None:\n",
    "        pad_id = tokenizer_op.get_token_id(\"<PAD>\")\n",
    "        ignore_token_value = -100\n",
    "        sample_dict[LABELS_STR] = (\n",
    "            f\"<@TOKENIZER-TYPE=AA><SENTINEL_ID_0><{solubility_label}><EOS>\"\n",
    "        )\n",
    "        tokenizer_op(\n",
    "            sample_dict=sample_dict,\n",
    "            key_in=LABELS_STR,\n",
    "            key_out_tokens_ids=LABELS_TOKENS,\n",
    "            key_out_attention_mask=LABELS_ATTENTION_MASK,\n",
    "            max_seq_len=labels_max_seq_len,\n",
    "        )\n",
    "        sample_dict[LABELS_TOKENS] = torch.tensor(\n",
    "            sample_dict[LABELS_TOKENS], device=device\n",
    "        )\n",
    "        sample_dict[LABELS_ATTENTION_MASK] = torch.tensor(\n",
    "            sample_dict[LABELS_ATTENTION_MASK], device=device\n",
    "        )\n",
    "        # replace pad_id with -100 to\n",
    "        pad_id_tns = torch.tensor(pad_id)\n",
    "        sample_dict[LABELS_TOKENS][\n",
    "            (sample_dict[LABELS_TOKENS][..., None] == pad_id_tns).any(-1).nonzero()\n",
    "        ] = ignore_token_value\n",
    "\n",
    "        sample_dict[DECODER_INPUTS_STR] = (\n",
    "            f\"<@TOKENIZER-TYPE=AA><DECODER_START><SENTINEL_ID_0><{solubility_label}><EOS>\"\n",
    "        )\n",
    "        tokenizer_op(\n",
    "            sample_dict=sample_dict,\n",
    "            key_in=DECODER_INPUTS_STR,\n",
    "            key_out_tokens_ids=DECODER_INPUTS_TOKENS,\n",
    "            key_out_attention_mask=DECODER_INPUTS_ATTENTION_MASK,\n",
    "            max_seq_len=labels_max_seq_len,\n",
    "        )\n",
    "        sample_dict[DECODER_INPUTS_TOKENS] = torch.tensor(\n",
    "            sample_dict[DECODER_INPUTS_TOKENS], device=device\n",
    "        )\n",
    "        sample_dict[DECODER_INPUTS_ATTENTION_MASK] = torch.tensor(\n",
    "            sample_dict[DECODER_INPUTS_ATTENTION_MASK], device=device\n",
    "        )\n",
    "\n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c99bcbe20f84736bf0205eb301c6bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Loading the tokenizer\n",
    "tokenizer_op = ModularTokenizerOp.from_pretrained(\"ibm/biomed.omics.bl.sm.ma-ted-458m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999], your vocabulary could be corrupted !\n",
      "Visualize participating sample dicts before and after the data processing for the model:\n",
      "initial_sample_dict={'data.protein_sequence': 'AAA'}\n",
      "processed_sample_dict={'data.protein_sequence': 'AAA', 'data.query.encoder_input': '<@TOKENIZER-TYPE=AA><MOLECULAR_ENTITY><MOLECULAR_ENTITY_GENERAL_PROTEIN><SOLUBILITY><SENTINEL_ID_0><@TOKENIZER-TYPE=AA@MAX-LEN=1250><SEQUENCE_NATURAL_START>AAA<SEQUENCE_NATURAL_END><EOS>', 'data.query.encoder_input.with_placeholders': '<@TOKENIZER-TYPE=AA><MOLECULAR_ENTITY><MOLECULAR_ENTITY_GENERAL_PROTEIN><SOLUBILITY><SENTINEL_ID_0><@TOKENIZER-TYPE=AA@MAX-LEN=1250><SEQUENCE_NATURAL_START>AAA<SEQUENCE_NATURAL_END><EOS>', 'data.query.encoder_input.per_meta_part_encoding': [Encoding(num_tokens=4, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])], 'data.encoder_input_token_ids': tensor([  6, 274,  27,  ...,   1,   1,   1]), 'data.encoder_input_attention_mask': tensor([ True,  True,  True,  ..., False, False, False])}\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "protein_sequence_key = \"data.protein_sequence\"\n",
    "\n",
    "# Define dummy initial sample dict\n",
    "initial_sample_dict = dict()\n",
    "initial_sample_dict[protein_sequence_key] = \"AAA\"\n",
    "\n",
    "processed_sample_dict = data_preprocessing(\n",
    "    copy.deepcopy(initial_sample_dict),\n",
    "    protein_sequence_key=protein_sequence_key,\n",
    "    tokenizer_op=tokenizer_op,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Visualize participating sample dicts before and after the data processing for the model:\"\n",
    ")\n",
    "print(f\"{initial_sample_dict=}\")\n",
    "print(f\"{processed_sample_dict=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `process_model_output()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def process_model_output(\n",
    "    tokenizer_op: ModularTokenizerOp,\n",
    "    decoder_output: np.ndarray,\n",
    "    decoder_output_scores: np.ndarray,\n",
    ") -> dict | None:\n",
    "    \"\"\"\n",
    "    Extract predicted solubility class and scores\n",
    "    expecting decoder output to be <SENTINEL_ID_0><0><EOS> or <SENTINEL_ID_0><1><EOS>\n",
    "    note - the normalized version will calculate the positive ('<1>') score divided by the sum of the scores for both '<0>' and '<1>'\n",
    "        BE CAREFUL as both negative and positive absolute scores can be drastically low, and normalized score could be very high.\n",
    "    outputs a dictionary containing:\n",
    "        dict(\n",
    "            predicted_token_str = #... e.g. '<1>'\n",
    "            not_normalized_score = #the score for the positive token... e.g.  0.01\n",
    "            normalized_score = #... (positive_token_score) / (positive_token_score+negative_token_score)\n",
    "        )\n",
    "        if there is any error in parsing the model output, None is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    negative_token_id = tokenizer_op.get_token_id(\"<0>\")\n",
    "    positive_token_id = tokenizer_op.get_token_id(\"<1>\")\n",
    "    label_id_to_int = {\n",
    "        negative_token_id: 0,\n",
    "        positive_token_id: 1,\n",
    "    }\n",
    "    classification_position = 1\n",
    "\n",
    "    if decoder_output_scores is not None:\n",
    "        not_normalized_score = decoder_output_scores[\n",
    "            classification_position, positive_token_id\n",
    "        ]\n",
    "        normalized_score = not_normalized_score / (\n",
    "            not_normalized_score\n",
    "            + decoder_output_scores[classification_position, negative_token_id]\n",
    "            + 1e-10\n",
    "        )\n",
    "    ans = dict(\n",
    "        pred=label_id_to_int.get(int(decoder_output[classification_position]), -1),\n",
    "        not_normalized_scores=not_normalized_score,\n",
    "        normalized_scores=normalized_score,\n",
    "    )\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "As part of the ML workflow, it is essential to evaluate tasks using standard metrics. This repository includes a minimal implementation for metrics collection, for both classification and regression tasks, provided in `mammal/metrics.py`. The metrics are based on the [fuse-med-ml](https://github.com/BiomedSciAI/fuse-med-ml/tree/master) package.\n",
    "\n",
    "**Classification metrics**: AUCROC, Accuracy, MCC (Matthews Correlation Coefficient ).\n",
    "**Regression metrics**: Pearson Correlation, Spearman Correlation, MAE, MSE, RMSE, R2.\n",
    "\n",
    "Since solubility is a classification problem (soluble or insoluble), we will utilize our classification metrics. Refer to the `classification_metrics` method in the code block below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Task Object\n",
    "\n",
    "Now we can combine all of the components together to form our `MammalTask` object that will define the new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from mammal.metrics import classification_metrics\n",
    "from mammal.task import (\n",
    "    MammalTask,\n",
    "    MetricBase,\n",
    ")\n",
    "\n",
    "\n",
    "class ProteinSolubilityTask(MammalTask):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        name: str,\n",
    "        tokenizer_op: ModularTokenizerOp,\n",
    "        logger: Any | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            logger=logger,\n",
    "            tokenizer_op=tokenizer_op,\n",
    "        )\n",
    "\n",
    "        self.preds_key = CLS_PRED\n",
    "        self.scores_key = SCORES\n",
    "        self.labels_key = LABELS_TOKENS\n",
    "\n",
    "    def data_module(self, **kwargs) -> pl.LightningDataModule:\n",
    "        return ProteinSolubilityDataModule(\n",
    "            tokenizer_op=self._tokenizer_op,\n",
    "            data_preprocessing=self.data_preprocessing,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def train_metrics(self) -> dict[str, MetricBase]:\n",
    "        metrics = super().train_metrics()\n",
    "        metrics.update(\n",
    "            classification_metrics(\n",
    "                self.name(),\n",
    "                class_position=1,\n",
    "                tokenizer_op=self._tokenizer_op,\n",
    "                class_tokens=[\"<0>\", \"<1>\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def validation_metrics(self) -> dict[str, MetricBase]:\n",
    "        validation_metrics = super().validation_metrics()\n",
    "        validation_metrics.update(\n",
    "            classification_metrics(\n",
    "                self.name(),\n",
    "                class_position=1,\n",
    "                tokenizer_op=self._tokenizer_op,\n",
    "                class_tokens=[\"<0>\", \"<1>\"],\n",
    "            )\n",
    "        )\n",
    "        return validation_metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def data_preprocessing(\n",
    "        sample_dict: dict,\n",
    "        *,\n",
    "        protein_sequence_key: str,\n",
    "        tokenizer_op: ModularTokenizerOp,\n",
    "        solubility_label_key: int | None = None,\n",
    "        protein_max_seq_length: int = 1250,\n",
    "        encoder_input_max_seq_len: int | None = 1260,\n",
    "        labels_max_seq_len: int | None = 4,\n",
    "        device: str | torch.device = \"cpu\",\n",
    "    ) -> dict:\n",
    "\n",
    "        # We use the method we defined above, just to make it cleaner\n",
    "        # Another option is to write the underling logic inside this function definition.\n",
    "        sample_dict = data_preprocessing(\n",
    "            sample_dict=sample_dict,\n",
    "            tokenizer_op=tokenizer_op,\n",
    "            protein_sequence_key=protein_sequence_key,\n",
    "            solubility_label_key=solubility_label_key,\n",
    "            protein_max_seq_length=protein_max_seq_length,\n",
    "            encoder_input_max_seq_len=encoder_input_max_seq_len,\n",
    "            labels_max_seq_len=labels_max_seq_len,\n",
    "            device=device,\n",
    "        )\n",
    "        return sample_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def process_model_output(\n",
    "        tokenizer_op: ModularTokenizerOp,\n",
    "        decoder_output: np.ndarray,\n",
    "        decoder_output_scores: np.ndarray,\n",
    "    ) -> dict | None:\n",
    "\n",
    "        # We use the method we defined above, just to make it cleaner\n",
    "        ans = process_model_output(\n",
    "            tokenizer_op=tokenizer_op,\n",
    "            decoder_output=decoder_output,\n",
    "            decoder_output_scores=decoder_output_scores,\n",
    "        )\n",
    "\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune model\n",
    "Now let's create a simple finetuning block using the task we just created. The following code is based on `main_finetune.py`, we you can run with your own task & configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 20:36:13.779182: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-23 20:36:13.803888: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732386973.823679  515207 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732386973.828829  515207 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-23 20:36:13.850546: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path doesn't exist. Will try to download fron hf hub. pretrained_model_name_or_path='ibm/biomed.omics.bl.sm.ma-ted-458m'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e577c25ec5c1468db34894b0b78a8544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load model from dir: pretrained_model_name_or_path='/dccstor/mm_hcls/usr/sagi/.cache/models--ibm--biomed.omics.bl.sm.ma-ted-458m/snapshots/421daf3f8eae4ada57ffd3580f7347828b34d69a'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size is 62478\n",
      "val set size is 6942\n",
      "test set size is 1999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type   | Params | Mode \n",
      "------------------------------------------\n",
      "0 | _model | Mammal | 458 M  | train\n",
      "------------------------------------------\n",
      "458 M     Trainable params\n",
      "0         Non-trainable params\n",
      "458 M     Total params\n",
      "1,832.029 Total estimated model params size (MB)\n",
      "579       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/mm_hcls/usr/sagi/envs/biomed_multi_alignment/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/dccstor/mm_hcls/usr/sagi/envs/biomed_multi_alignment/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fdb7a1f0b74e27aa1f42e6cfb0fb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "from fuse.dl.lightning.pl_module import LightningModuleDefault\n",
    "\n",
    "from mammal.model import Mammal\n",
    "from mammal.lr_schedulers import cosine_annealing_with_warmup_lr_scheduler\n",
    "\n",
    "# Seed for reproducibility \n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Load pre-trained model from HF hub\n",
    "model = Mammal.from_pretrained(\"ibm/biomed.omics.bl.sm.ma-ted-458m\")\n",
    "\n",
    "# Initialized OUR task :-)\n",
    "task = ProteinSolubilityTask(name=\"our_solubility_prediction_task\", tokenizer_op=tokenizer_op)\n",
    "\n",
    "# Extract Task's datamodule to pass it to the Lightning module\n",
    "pl_data_module = task.data_module()\n",
    "\n",
    "# Define optimizer and lr scheduler for the Lightning module\n",
    "opt = AdamW(params=model.parameters(), lr=0.00001)\n",
    "lr_scheduler = cosine_annealing_with_warmup_lr_scheduler(optimizer=opt)\n",
    "optimizers_and_lr_schs = dict(\n",
    "        optimizer=opt,\n",
    "        lr_scheduler={\"scheduler\": lr_scheduler, \"interval\": \"step\"},\n",
    ")\n",
    "\n",
    "# Initialize 'LightningModuleDefault' which is a subclass of 'pl.LightningModule' defined in Fuse-Med-ML.\n",
    "pl_module = LightningModuleDefault( model=model,\n",
    "        losses=task.losses(),\n",
    "        validation_metrics=task.validation_metrics(),\n",
    "        train_metrics=task.train_metrics(),\n",
    "        optimizers_and_lr_schs=optimizers_and_lr_schs,\n",
    "        model_dir=\"mammal_solubility_finetune\",\n",
    ")\n",
    "\n",
    "# Create Lightning's Trainer\n",
    "pl_trainer = pl.Trainer(max_epochs=1)\n",
    "\n",
    "# Let the training begin\n",
    "pl_trainer.fit(model=pl_module, datamodule=pl_data_module)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomed-multi-alignment",
   "language": "python",
   "name": "biomed-multi-alignment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
