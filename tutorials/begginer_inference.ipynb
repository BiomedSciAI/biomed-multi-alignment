{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using MAMMAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install `biomed-multi-alignment` package. One can also clone and install it in editable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install biomed-multi-alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run simplest inference script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fuse.data.tokenizers.modular_tokenizer.op import ModularTokenizerOp\n",
    "from mammal.model import Mammal\n",
    "from mammal.keys import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Model and set it to evaluation mode\n",
    "model = Mammal.from_pretrained(\"ibm/biomed.omics.bl.sm.ma-ted-458m\")\n",
    "model.eval()\n",
    "model.to(device=device)\n",
    "\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer_op = ModularTokenizerOp.from_pretrained(\"ibm/biomed.omics.bl.sm.ma-ted-458m\")\n",
    "\n",
    "# Prepare Input Prompt\n",
    "protein_calmodulin = \"MADQLTEEQIAEFKEAFSLFDKDGDGTITTKELGTVMRSLGQNPTEAELQDMISELDQDGFIDKEDLHDGDGKISFEEFLNLVNKEMTADVDGDGQVNYEEFVTMMTSK\"\n",
    "protein_calcineurin = \"MSSKLLLAGLDIERVLAEKNFYKEWDTWIIEAMNVGDEEVDRIKEFKEDEIFEEAKTLGTAEMQEYKKQKLEEAIEGAFDIFDKDGNGYISAAELRHVMTNLGEKLTDEEVDEMIRQMWDQNGDWDRIKELKFGEIKKLSAKDTRGTIFIKVFENLGTGVDSEYEDVSKYMLKHQ\"\n",
    "\n",
    "# Create and load sample\n",
    "sample_dict = dict()\n",
    "# Formatting prompt to match pre-training syntax\n",
    "sample_dict[ENCODER_INPUTS_STR] = f\"<@TOKENIZER-TYPE=AA><BINDING_AFFINITY_CLASS><SENTINEL_ID_0><MOLECULAR_ENTITY><MOLECULAR_ENTITY_GENERAL_PROTEIN><SEQUENCE_NATURAL_START>{protein_calmodulin}<SEQUENCE_NATURAL_END><MOLECULAR_ENTITY><MOLECULAR_ENTITY_GENERAL_PROTEIN><SEQUENCE_NATURAL_START>{protein_calcineurin}<SEQUENCE_NATURAL_END><EOS>\"\n",
    "\n",
    "# Tokenize\n",
    "tokenizer_op(\n",
    "    sample_dict=sample_dict,\n",
    "    key_in=ENCODER_INPUTS_STR,\n",
    "    key_out_tokens_ids=ENCODER_INPUTS_TOKENS,\n",
    "    key_out_attention_mask=ENCODER_INPUTS_ATTENTION_MASK,\n",
    ")\n",
    "sample_dict[ENCODER_INPUTS_TOKENS] = torch.tensor(sample_dict[ENCODER_INPUTS_TOKENS]).to(device=device)\n",
    "sample_dict[ENCODER_INPUTS_ATTENTION_MASK] = torch.tensor(sample_dict[ENCODER_INPUTS_ATTENTION_MASK]).to(device=device)\n",
    "\n",
    "# Generate Prediction\n",
    "batch_dict = model.generate(\n",
    "    [sample_dict],\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    max_new_tokens=5,\n",
    ")\n",
    "\n",
    "# Get output\n",
    "generated_output = tokenizer_op._tokenizer.decode(batch_dict[CLS_PRED][0])\n",
    "print(f\"{generated_output=}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
